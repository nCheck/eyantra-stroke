{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/data/output.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-09cee3d5c499>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mtestData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtestY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/data/output.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mtestData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stroke'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stroke'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mtrainData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/data/train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'/data/output.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# Project Name: AI Based Early Stroke Detection \n",
    "# Author List: Nehal Kalnad,Ashley Lobo, e-Yantra Team \n",
    "# Filename: neural-network.ipynb\n",
    "# Functions: imblearn.over_sampling.smote.fit_sample ,sklearn.preprocessing.StandardScaler, sklearn.neural_network.MLPClassifier.predict\n",
    "# Global Variables:testData,testY,trainData,temp,scaler,trainDataNS,trainDataS,testDataNS,testDataS,xTrainDataNS,yTrainDataNS,model,predictions, \n",
    "#                  xTrainDataS,yTrainDataS,smokeCol,nsmokeCol,x_train,x_test,y_train,y_test,x_trainS,x_testS,y_trainS,y_testS\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Plot\n",
    "import matplotlib.pyplot as plt \n",
    "plt.rc(\"font\", size=14)\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\") #white background style for seaborn plots\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "# Data set obtained from\n",
    "# https://www.kaggle.com/asaumya/healthcare-dataset-stroke-data\n",
    "\n",
    "testData = pd.read_csv(\"data/test.csv\")\n",
    "testY = pd.read_csv(\"data/output.csv\")\n",
    "testData['stroke'] = testY['stroke']\n",
    "trainData = pd.read_csv(\"/data/train.csv\")\n",
    "\n",
    "#Not Significant for Strokes\n",
    "\n",
    "trainData = trainData.drop( columns=['ever_married', 'gender', 'id','avg_glucose_level'] )\n",
    "testData = testData.drop( columns=['ever_married', 'gender', 'id','avg_glucose_level'] )\n",
    "\n",
    "#Filling Missing BMI Data === train\n",
    "\n",
    "temp = trainData['bmi'].mean() + trainData['bmi'].median()\n",
    "temp /= 2\n",
    "temp\n",
    "trainData['bmi'] = trainData['bmi'].fillna(temp)\n",
    "\n",
    "#Filling Missing BMI Data ===test\n",
    "\n",
    "temp = testData['bmi'].mean() + testData['bmi'].median()\n",
    "temp /= 2\n",
    "temp\n",
    "testData['bmi'] = testData['bmi'].fillna(temp)\n",
    "\n",
    "# Segregating into Two Models Smoking & Non-Smoking\n",
    "\n",
    "\n",
    "trainDataS = trainData[ trainData['smoking_status'].notna() ]\n",
    "testDataS = testData[ testData['smoking_status'].notna() ]\n",
    "\n",
    "# X and Y division\n",
    "\n",
    "yTrainDataS = trainDataS['stroke']\n",
    "xTrainDataS = trainDataS.drop( columns=['stroke'] )\n",
    "\n",
    "yTestDataS = testDataS['stroke']\n",
    "xTestDataS = testDataS.drop( columns=['stroke'] )\n",
    "\n",
    "#Nominal Categories to one-hot encoding === Train\n",
    "\n",
    "#Smoke Data\n",
    "xTrainDataS = pd.get_dummies( xTrainDataS, \n",
    "                             columns=[ 'work_type' , 'Residence_type', 'smoking_status']\n",
    "                            , prefix= ['work_type' , 'res_type', 'smoke'] )\n",
    "\n",
    "\n",
    "#Nominal Categories to one-hot encoding ===Test\n",
    "\n",
    "#Smoke Data\n",
    "xTestDataS = pd.get_dummies( xTestDataS,\n",
    "                            columns=['work_type' , 'Residence_type', 'smoking_status']\n",
    "                            , prefix=['work_type' , 'res_type', 'smoke'] )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "smokeCol = xTrainDataS.columns\n",
    "nsmokeCol = xTrainDataNS.columns\n",
    "\n",
    "\n",
    "#Sampling for removing imbalance\n",
    "\n",
    "\n",
    "# Function Name:imblearn.over_sampling.smote.fit_sample \n",
    "# Input:X:{array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "#               y:array-like, shape (n_samples,) \n",
    "# Output:X_resampled:{array-like, sparse matrix}, shape (n_samples_new, n_features) \n",
    "#               y_resampled:array-like, shape (n_samples_new,) \n",
    "# Logic:Used to balance dataset\n",
    "# Example Call: X_res, y_res = sm.fit_sample(X, y) \n",
    "\n",
    "from imblearn.over_sampling import SMOTE,RandomOverSampler,ADASYN\n",
    "smote = RandomOverSampler(random_state=0)\n",
    "\n",
    "\n",
    "xTrainDataS, yTrainDataS = smote.fit_sample(xTrainDataS, yTrainDataS)\n",
    "\n",
    "xTrainDataS = pd.DataFrame( xTrainDataS , columns=smokeCol )\n",
    "yTrainDataS = pd.DataFrame( yTrainDataS , columns=['stroke'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Train Splitter\n",
    "\n",
    "x_trainS, x_testS, y_trainS, y_testS = train_test_split(xTrainDataS, np.ravel( yTrainDataS , order='C' ),\n",
    "                                                        test_size=0.35, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "\n",
    "# Function Name:sklearn.preprocessing.StandardScaler, \n",
    "# Input: X : array-like, shape [n_samples, n_features]         \n",
    "# Output:\n",
    "# Logic:Perform standardization by centering and scaling\n",
    "# Example Call: scaler.transform(x_test)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_trainS)\n",
    "\n",
    "x_trainS = scaler.transform(x_trainS)\n",
    "x_testS = scaler.transform(x_testS)\n",
    "xTestDataS = scaler.transform( xTestDataS )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MLP\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "sModel = MLPClassifier( hidden_layer_sizes=(16,64,64) , max_iter=300 , alpha=0.00008 )\n",
    "\n",
    "\n",
    "sModel.fit(x_trainS, y_trainS)\n",
    "\n",
    "\n",
    "# Function Name:sklearn.neural_network.MLPClassifier.predict, \n",
    "# Input:X : {array-like, sparse matrix}, shape (n_samples, n_features)              \n",
    "# Output:y : array-like, shape (n_samples,) or (n_samples, n_classes)\n",
    "# Logic:Predict using the multi-layer perceptron classifier\n",
    "# Example Call: X_res= sModel.preditc(X), \n",
    "\n",
    "#Synthetic Test\n",
    "\n",
    "predictions = sModel.predict(x_testS)\n",
    "print(sModel.score(x_testS, y_testS))\n",
    "print(metrics.confusion_matrix(y_testS, predictions))\n",
    "\n",
    "\n",
    "#Blind Test\n",
    "\n",
    "predictions = sModel.predict(xTestDataS)\n",
    "print(sModel.score(xTestDataS, yTestDataS))\n",
    "print(metrics.confusion_matrix(yTestDataS, predictions))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    " \n",
    "0.9208395947452616\n",
    "[[ 8687  1581]\n",
    " [   52 10309]]\n",
    "0.6601556420233463\n",
    "[[7998 1445]\n",
    " [2922  485]]\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Creates Model for Flask running\n",
    "\n",
    "from joblib import dump\n",
    "\n",
    "dump( sModel , 'smokemlp.joblib' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
